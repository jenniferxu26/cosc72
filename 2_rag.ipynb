{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# 2_rag.ipynb\n",
    "\n",
    "Jennifer Xu (Jennifer.Xu.26@dartmouth.edu)\n",
    "\n",
    "This retrieval-augmented generation pipeline takes train.jsonl and the Chinese dictionaries as input. It combines the dictionaries into a single lookup table, then uses Jieba tokenization to scan each poem line and retrieve English glosses. The resulting dataset is saved as train_gloss.jsonl."
   ],
   "id": "25a42d30e865253f"
  },
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2025-06-10T02:26:34.991089Z",
     "start_time": "2025-06-10T02:26:34.978581Z"
    }
   },
   "source": [
    "from pathlib import Path\n",
    "import urllib.request, gzip, shutil, os, json, re, jieba, unicodedata, random, textwrap\n",
    "\n",
    "RAW_DIR = Path(\"data/raw\")\n",
    "PROC_DIR = Path(\"data/proc\")\n",
    "RAW_DIR.mkdir(parents=True, exist_ok=True)"
   ],
   "outputs": [],
   "execution_count": 1
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Build {word: short_gloss} dictionary",
   "id": "774fd1672b578917"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-10T02:26:35.462917Z",
     "start_time": "2025-06-10T02:26:35.029068Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from collections import defaultdict\n",
    "\n",
    "# load CC-CEDICT\n",
    "cedict = {}\n",
    "with open(\"data/raw/cedict_ts.u8\", encoding=\"utf-8\") as f:\n",
    "    for ln in f:\n",
    "        if ln.startswith(\"#\"): continue\n",
    "        parts = ln.strip().split(\" \", 2)\n",
    "        if len(parts) != 3: continue\n",
    "        trad, simp, rest = parts\n",
    "        gloss = rest.split(\"/\")[1] if \"/\" in rest else \"\"\n",
    "        if simp and gloss:\n",
    "            cedict.setdefault(simp, gloss)\n",
    "\n",
    "# merge idioms & expressions\n",
    "for fname, key in [(\"idiom.json\", \"word\"), (\"ci.json\", \"ci\")]:\n",
    "    for obj in json.loads((RAW_DIR / fname).read_text(encoding=\"utf-8\")):\n",
    "        word  = obj[key]\n",
    "        gloss = obj.get(\"explanation\", obj.get(\"derivation\", \"\"))[:60]\n",
    "        if word not in cedict and gloss:\n",
    "            cedict[word] = gloss\n",
    "\n",
    "print(f\"Combined dictionary size: {len(cedict):,}\")"
   ],
   "id": "248ebd5f68a0b981",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Combined dictionary size: 359,040\n"
     ]
    }
   ],
   "execution_count": 2
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Regex",
   "id": "2ce4bfba864617ee"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-10T02:26:35.524445Z",
     "start_time": "2025-06-10T02:26:35.520332Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# 4-char idioms\n",
    "IDIOM_RE = re.compile(r\"[\\u4e00-\\u9fff]{4}\")\n",
    "# none = match all\n",
    "MAX_GLOSSES = 3\n",
    "\n",
    "def collect_glosses(zh: str) -> str:\n",
    "    zh = unicodedata.normalize(\"NFKC\", zh)\n",
    "    found, seen = [], set()\n",
    "\n",
    "    for ido in IDIOM_RE.findall(zh):\n",
    "        if ido in cedict:\n",
    "            found.append(cedict[ido]);  seen.add(ido)\n",
    "\n",
    "    for tok in jieba.cut(zh):\n",
    "        if tok in cedict and tok not in seen:\n",
    "            found.append(cedict[tok]);  seen.add(tok)\n",
    "\n",
    "    if MAX_GLOSSES is not None:\n",
    "        found = found[:MAX_GLOSSES]\n",
    "\n",
    "    return \"; \".join(found)"
   ],
   "id": "6e6151d09e32fb33",
   "outputs": [],
   "execution_count": 3
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Attach glosses to train/test",
   "id": "121ed56217d16732"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-10T02:26:35.677784Z",
     "start_time": "2025-06-10T02:26:35.673780Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def augment_file(in_path, out_path):\n",
    "    total = hit = 0\n",
    "    with open(in_path, encoding=\"utf-8\") as fin, \\\n",
    "         open(out_path, \"w\", encoding=\"utf-8\") as fout:\n",
    "        for ln in fin:\n",
    "            rec = json.loads(ln)\n",
    "            if rec[\"zh\"]:\n",
    "                g = collect_glosses(rec[\"zh\"])\n",
    "                if g:\n",
    "                    hit += 1\n",
    "                rec[\"gloss\"] = g\n",
    "            fout.write(json.dumps(rec, ensure_ascii=False) + \"\\n\")\n",
    "            total += 1\n",
    "    pct = hit / total * 100\n",
    "    print(f\"{in_path.name}: {hit}/{total} lines glossed ({pct:.1f} %)\")"
   ],
   "id": "b252b400ca393e1",
   "outputs": [],
   "execution_count": 4
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-10T02:26:36.147581Z",
     "start_time": "2025-06-10T02:26:35.753331Z"
    }
   },
   "cell_type": "code",
   "source": "augment_file(PROC_DIR / \"train.jsonl\", PROC_DIR / \"train_gloss.jsonl\")",
   "id": "53e2c889f6b28cf6",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Building prefix dict from C:\\Users\\bb\\anaconda3\\Lib\\site-packages\\jieba\\dict.txt ...\n",
      "Loading model from cache C:\\Users\\bb\\AppData\\Local\\Temp\\jieba.cache\n",
      "Loading model cost 0.3609955310821533 seconds.\n",
      "Prefix dict has been built succesfully.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train.jsonl: 834/904 lines glossed (92.3 %)\n"
     ]
    }
   ],
   "execution_count": 5
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-10T02:26:36.213830Z",
     "start_time": "2025-06-10T02:26:36.204833Z"
    }
   },
   "cell_type": "code",
   "source": [
    "## check\n",
    "import itertools, textwrap, json\n",
    "with open(PROC_DIR / \"train_gloss.jsonl\", encoding=\"utf-8\") as f:\n",
    "    for ln in f:\n",
    "        rec = json.loads(ln)\n",
    "        if rec[\"gloss\"]:\n",
    "            print(textwrap.indent(json.dumps(rec, ensure_ascii=False, indent=2), \"  \"))\n",
    "            break"
   ],
   "id": "e8f89099f77d44a",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  {\n",
      "    \"zh\": \"徒此揖清芬\",\n",
      "    \"en\": \"We can but breathe your fragrance the wind brings down.\",\n",
      "    \"gloss\": \"to greet by raising clasped hands; 1.清香。 \\n2.喻高洁的德行。\"\n",
      "  }\n"
     ]
    }
   ],
   "execution_count": 6
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
